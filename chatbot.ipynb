{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gjkGMW8AT4BI",
        "d0ppNDC5UZfc",
        "OlLWxWkQLnEv",
        "93Utu600hX35",
        "IMBXKmF1lVJO"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfYjmIFaTvQg"
      },
      "source": [
        "# **BUILDING A CHATBOT WITH DEEP NLP**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjkGMW8AT4BI"
      },
      "source": [
        "## **IMPORTING THE LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2JZ3PyCT-d6"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq9_lA0TUIoP",
        "outputId": "fd019d94-5859-4a71-b8b9-1ff226eeef4c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ppNDC5UZfc"
      },
      "source": [
        "## **STEP 1: DATA PREPROCESSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHPEUKm-UecM"
      },
      "source": [
        "### **IMPORTING THE DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1DFSCl8Ugyc"
      },
      "source": [
        "path = '/content/drive/My Drive'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI7O5pluVIcr"
      },
      "source": [
        "lines = open(path+'/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open(path+'/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfe0NAxZbNUJ",
        "outputId": "02d87e4c-56ce-44b7-8cda-63862e345f36"
      },
      "source": [
        "lines[0:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
              " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
              " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
              " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
              " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\",\n",
              " 'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow',\n",
              " \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\",\n",
              " 'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No',\n",
              " 'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?',\n",
              " 'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBnDBPs7bPoS",
        "outputId": "807c4c24-59dc-4ecd-dce7-fc2706b5633d"
      },
      "source": [
        "conversations[0:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAjqikVuVkTr"
      },
      "source": [
        "### **CREATING A DICTIONARY THAT MAPS EACH LINES AND ITS ID**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMYOXvUaVbgb"
      },
      "source": [
        "id2line = {}\n",
        "\n",
        "for line in lines :\n",
        "  # Splitting the line by the code word `+++$+++`\n",
        "  _line = line.split(' +++$+++ ') \n",
        "  if len(_line) == 5 : \n",
        "      id2line[_line[0]] = _line[4]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWcae1fTX6cU"
      },
      "source": [
        "### **CREATING A LIST OF ALL OF THE CONVERSATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kxKSUiIX1xm"
      },
      "source": [
        "conversations_ids = []\n",
        "\n",
        "for conversation in conversations[:-1] : \n",
        "  _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "  conversations_ids.append(_conversation.split(','))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_IWnldiZiyv",
        "outputId": "3574ed06-15b2-4db5-fef5-490b1828b2f2"
      },
      "source": [
        "conversations_ids[0:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['L194', 'L195', 'L196', 'L197'],\n",
              " ['L198', 'L199'],\n",
              " ['L200', 'L201', 'L202', 'L203'],\n",
              " ['L204', 'L205', 'L206'],\n",
              " ['L207', 'L208'],\n",
              " ['L271', 'L272', 'L273', 'L274', 'L275'],\n",
              " ['L276', 'L277'],\n",
              " ['L280', 'L281'],\n",
              " ['L363', 'L364'],\n",
              " ['L365', 'L366']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GAGeHm8ZybJ"
      },
      "source": [
        "### **GET THE QUESTIONS AND THE ANSWERS SEPARATELY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_Bg9-e4Z2QF"
      },
      "source": [
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for conversation in conversations_ids :\n",
        "  for i in range (len(conversation) - 1) : \n",
        "    questions.append(id2line[conversation[i]])\n",
        "    answers.append(id2line[conversation[i+1]])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW8yUeIZaw78",
        "outputId": "c373736d-fb42-40e5-d43c-a9c183150b03"
      },
      "source": [
        "questions[0:20]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
              " \"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
              " 'Not the hacking and gagging and spitting part.  Please.',\n",
              " \"You're asking me out.  That's so cute. What's your name again?\",\n",
              " \"No, no, it's my fault -- we didn't have a proper introduction ---\",\n",
              " 'Cameron.',\n",
              " \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n",
              " 'Why?',\n",
              " 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
              " 'Gosh, if only we could find Kat a boyfriend...',\n",
              " \"C'esc ma tete. This is my head\",\n",
              " \"Right.  See?  You're ready for the quiz.\",\n",
              " \"I don't want to know how to say that though.  I want to know useful things. Like where the good stores are.  How much does champagne cost?  Stuff like Chat.  I have never in my life had to point out my head to someone.\",\n",
              " \"That's because it's such a nice one.\",\n",
              " 'How is our little Find the Wench A Date plan progressing?',\n",
              " 'There.',\n",
              " 'You got something on your mind?',\n",
              " 'You have my word.  As a gentleman',\n",
              " 'How do you get your hair to look like that?',\n",
              " 'Sure have.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PIAMsqda4ip",
        "outputId": "81b8f189-b57b-4e62-f873-17f87d4ab9fe"
      },
      "source": [
        "answers[0:20]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
              " 'Not the hacking and gagging and spitting part.  Please.',\n",
              " \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\",\n",
              " 'Forget it.',\n",
              " 'Cameron.',\n",
              " \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n",
              " 'Seems like she could get a date easy enough...',\n",
              " 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
              " \"That's a shame.\",\n",
              " 'Let me see what I can do.',\n",
              " \"Right.  See?  You're ready for the quiz.\",\n",
              " \"I don't want to know how to say that though.  I want to know useful things. Like where the good stores are.  How much does champagne cost?  Stuff like Chat.  I have never in my life had to point out my head to someone.\",\n",
              " \"That's because it's such a nice one.\",\n",
              " 'Forget French.',\n",
              " \"Well, there's someone I think might be --\",\n",
              " 'Where?',\n",
              " \"I counted on you to help my cause. You and that thug are obviously failing. Aren't we ever going on our date?\",\n",
              " \"You're sweet.\",\n",
              " \"Eber's Deep Conditioner every two days. And I never, ever use a blowdryer without the diffuser attachment.\",\n",
              " \"I really, really, really wanna go, but I can't.  Not unless my sister goes.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_ZJ2rO8bk2c"
      },
      "source": [
        "### **CLEANING ALL THE TEXTS FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZr4sNCbboPm"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower() # to lower case every letter in the text\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
        "    return text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8joFvfFWd-39"
      },
      "source": [
        "### **CLEANING THE QUESTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ykaQrLfd9X7"
      },
      "source": [
        "clean_questions = []\n",
        "\n",
        "for question in questions : \n",
        "  clean_questions.append(clean_text(question))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBSzp9KQehFX",
        "outputId": "b370e95b-ca66-4d26-9572-b0e69c9ab6f4"
      },
      "source": [
        "clean_questions[0:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again',\n",
              " 'well i thought we would start with pronunciation if that is okay with you',\n",
              " 'not the hacking and gagging and spitting part  please',\n",
              " 'you are asking me out  that is so cute what is your name again',\n",
              " \"no no it's my fault  we didn't have a proper introduction \",\n",
              " 'cameron',\n",
              " 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does',\n",
              " 'why',\n",
              " 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something',\n",
              " 'gosh if only we could find kat a boyfriend']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbGszFk5eW3a"
      },
      "source": [
        "### **CLEANING THE ANSWERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tSXcwBBeZRT"
      },
      "source": [
        "clean_answers = []\n",
        "\n",
        "for answer in answers : \n",
        "  clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftTpHEKmej0n",
        "outputId": "1e4d131f-4d1c-43e3-cd36-ac168d605cf6"
      },
      "source": [
        "clean_answers[0:10]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['well i thought we would start with pronunciation if that is okay with you',\n",
              " 'not the hacking and gagging and spitting part  please',\n",
              " \"okay then how 'bout we try out some french cuisine  saturday  night\",\n",
              " 'forget it',\n",
              " 'cameron',\n",
              " 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does',\n",
              " 'seems like she could get a date easy enough',\n",
              " 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something',\n",
              " 'that is a shame',\n",
              " 'let me see what i can do']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtFQEBsGjd_B"
      },
      "source": [
        "### **REMOVING THE NON FREQUENT WORDS OCCURING BY CREATING A CORPUS**\n",
        "\n",
        "**CREATING A DICTIONARY THAT MAPS EACH WORD TO ITS NUMBER OF OCCURANCES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWTkD453jlpC"
      },
      "source": [
        "word2count = {}\n",
        "\n",
        "for question in clean_questions : \n",
        "  for word in question.split() :\n",
        "    if word not in word2count : \n",
        "      word2count[word] = 1\n",
        "    else :\n",
        "      word2count[word] += 1\n",
        "\n",
        "for answer in clean_answers : \n",
        "  for word in answer.split() :\n",
        "    if word not in word2count : \n",
        "      word2count[word] = 1\n",
        "    else :\n",
        "      word2count[word] += 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5XNMFzmjnoZ"
      },
      "source": [
        "### **CREATING TWO DICTIONARIES THAT MAP THE QUESTIONS WORDS AND ANSWERS WORDS TO A UNIQUE INTEGERS**\n",
        "\n",
        "**TOKENISATION AND FILTERING THE NON FREQUENT WORDS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMktzoUBjs7a"
      },
      "source": [
        "threshold = 20\n",
        "questionsWords2int = {}\n",
        "word_number = 0\n",
        "\n",
        "for word, count in word2count.items() :\n",
        "  if count >= threshold : \n",
        "    questionsWords2int[word] = word_number\n",
        "    word_number += 1\n",
        "\n",
        "answersWords2int = {}   \n",
        "word_number = 0\n",
        "\n",
        "for word, count in word2count.items() :\n",
        "  if count >= threshold : \n",
        "    answersWords2int[word] = word_number\n",
        "    word_number += 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6vu3JQjkg_p"
      },
      "source": [
        "**ADDING THE LAST TOKENS TO THESE DICTIONARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heNFO8iCklvg"
      },
      "source": [
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "\n",
        "for token in tokens:\n",
        "    questionsWords2int[token] = len(questionsWords2int) + 1\n",
        "for token in tokens:\n",
        "    answersWords2int[token] = len(answersWords2int) + 1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOTXpJ9EyKix"
      },
      "source": [
        "### **CREATING THE INVERSE DICTIONARY OF THE ANSWERSWORDS2INT DICTIONARY**\n",
        "\n",
        "This is to inverse map the integers in the answerWords2int dictionary to individual words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVeKrfSOyZQD"
      },
      "source": [
        "answersInt2Words = {w_i : w for w, w_i in answersWords2int.items()}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpSyG9hqzNu6"
      },
      "source": [
        "### **ADDING THE `<EOS>` TOKEN TO THE END OF EVERY ANSWERS IN THE LIST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKRxTxTIzTTp"
      },
      "source": [
        "for i in range (len(clean_answers)) :\n",
        "  clean_answers[i] += ' <EOS>' "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81b3Piulz23B",
        "outputId": "42325fa7-d777-4218-cd81-9381f5110301"
      },
      "source": [
        "clean_answers[0:10]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['well i thought we would start with pronunciation if that is okay with you <EOS>',\n",
              " 'not the hacking and gagging and spitting part  please <EOS>',\n",
              " \"okay then how 'bout we try out some french cuisine  saturday  night <EOS>\",\n",
              " 'forget it <EOS>',\n",
              " 'cameron <EOS>',\n",
              " 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does <EOS>',\n",
              " 'seems like she could get a date easy enough <EOS>',\n",
              " 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something <EOS>',\n",
              " 'that is a shame <EOS>',\n",
              " 'let me see what i can do <EOS>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBNNKbdBz2Sx"
      },
      "source": [
        "### **TRANSLATING ALL THE QUESTIONS AND ANSWERS TO INTEGERS AND REPLACING ALL THE WORDS WHICH WERE FILTERED OUT BY `<OUT>`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq5uiCau02SS"
      },
      "source": [
        "questions_to_int = []\n",
        "\n",
        "for question in clean_questions :\n",
        "  ints = []\n",
        "  for word in question.split() :\n",
        "    if word not in questionsWords2int :\n",
        "      ints.append(questionsWords2int['<OUT>'])\n",
        "    else :\n",
        "      ints.append(questionsWords2int[word])\n",
        "  questions_to_int.append(ints)      "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFp1jEVX2IU3"
      },
      "source": [
        "answers_to_int = []\n",
        "\n",
        "for answer in clean_answers :\n",
        "  ints = []\n",
        "  for word in answer.split() :\n",
        "    if word not in answersWords2int :\n",
        "      ints.append(answersWords2int['<OUT>'])\n",
        "    else :\n",
        "      ints.append(answersWords2int[word])\n",
        "  answers_to_int.append(ints) "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqBf22Ew2sZx"
      },
      "source": [
        "### **SORTING THE QUESTIONS AND ANSWERS BY THE LENGTH OF THE QUESTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAZaF6hW2x-q"
      },
      "source": [
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "\n",
        "# Min length = 1, Max length = 24\n",
        "for length in range (1, 25) : \n",
        "  for i in enumerate(questions_to_int) :\n",
        "    if len(i[1]) == length :\n",
        "      sorted_clean_questions.append(questions_to_int[i[0]])\n",
        "      sorted_clean_answers.append(answers_to_int[i[0]])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwVCY8Om4dQf",
        "outputId": "e94beacf-c47c-40e5-84e4-3a25c83857c5"
      },
      "source": [
        "sorted_clean_questions[0:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[47], [62], [123], [147], [135], [39], [175], [39], [182], [183]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TjBBe2V4fmr",
        "outputId": "fe73ac7a-a978-4d8d-980d-6fe833bc5467"
      },
      "source": [
        "sorted_clean_answers[0:10]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[15,\n",
              "  48,\n",
              "  25,\n",
              "  47,\n",
              "  18,\n",
              "  49,\n",
              "  50,\n",
              "  15,\n",
              "  51,\n",
              "  52,\n",
              "  45,\n",
              "  53,\n",
              "  8824,\n",
              "  54,\n",
              "  52,\n",
              "  55,\n",
              "  41,\n",
              "  56,\n",
              "  18,\n",
              "  57,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  8823],\n",
              " [8824,\n",
              "  63,\n",
              "  60,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  69,\n",
              "  60,\n",
              "  70,\n",
              "  71,\n",
              "  72,\n",
              "  73,\n",
              "  74,\n",
              "  75,\n",
              "  76,\n",
              "  77,\n",
              "  60,\n",
              "  78,\n",
              "  79,\n",
              "  52,\n",
              "  74,\n",
              "  80,\n",
              "  81,\n",
              "  8823],\n",
              " [102, 8823],\n",
              " [1529, 77, 101, 1550, 33, 149, 608, 8823],\n",
              " [27, 153, 227, 3, 6453, 8823],\n",
              " [26, 27, 7, 160, 253, 65, 1280, 97, 65, 613, 8823],\n",
              " [1387, 134, 8823],\n",
              " [27, 239, 133, 194, 226, 74, 8823],\n",
              " [196, 8823],\n",
              " [20, 27, 124, 612, 32, 45, 1512, 47, 8823]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlLWxWkQLnEv"
      },
      "source": [
        "## **STEP 2: BUILDING THE NLP SEQUENCE TO SEQUENCE MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsedleO9Lt2A"
      },
      "source": [
        "### **CREATING PLACEHOLDERS FOR THE INPUTS AND THE TARGETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQydvsUSLsln"
      },
      "source": [
        "def model_inputs() :\n",
        "  inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
        "  targets = tf.placeholder(tf.int32, [None, None], name = 'target') \n",
        "  lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
        "  keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
        "  return inputs, targets, lr, keep_prob"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XcV0-0yNnPf"
      },
      "source": [
        "### **PREPROCESSING THE TARGETS INTO BATCHES OF SOME SIZE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3WfzXt2NeNa"
      },
      "source": [
        "def preprocess_targets(targets, word2int, batch_size) :\n",
        "  # First word should be a starting token\n",
        "  left_side = tf.fill([batch_size, 1], word2int['<SOS>']) \n",
        "  # Stride and slide each target\n",
        "  right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1]) \n",
        "  preprocessed_targets = tf.concat([left_side, right_side], axis = 1)\n",
        "  return preprocessed_targets"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7nFQnjjQwuI"
      },
      "source": [
        "### **CREATING THE ENCODING RNN LAYER { STACKED LSTM }**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   RNN Inputs: Inputs, targets, lr etc.\n",
        "*   RNN Size: Number of input tensors\n",
        "*   RNN Layers\n",
        "*   Keep_Prob: To control the dropout rate\n",
        "*   Sequence_Length: length of questions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxffGzoRQ5SI"
      },
      "source": [
        "def encoder_rnn(rnn_inputs, rnn_size, rnn_layers, keep_prob, sequence_length) : \n",
        "  lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "  lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "  encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "  # Input and ouput size of the forward and backward cell must match\n",
        "  _, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fv = encoder_cell,\n",
        "                                                                  cell_bv = encoder_cell,\n",
        "                                                                  sequence_length = sequence_length,\n",
        "                                                                  inputs = rnn_inputs,\n",
        "                                                                  dtype = tf.float32\n",
        "                                                                  ) \n",
        "  return encoder_state                                                                 "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa44Osn5Uagy"
      },
      "source": [
        "### **DECODING THE OBSERVATIONS ON THE TRAINING SET**\n",
        "\n",
        "\n",
        "\n",
        "*   Encoder state: output from encoder_rnn_layer\n",
        "*   Decoder cell: cell in the rnn decoder\n",
        "*   Decoder embedded unit: unit on which embedding is applied on \n",
        "*   Sequence_length\n",
        "*   Decoding_Scope: ds that wraps tf variables\n",
        "*   Output_function: fn used to return decoder o/p\n",
        "*   Keep Prob\n",
        "*   Batch_size\n",
        "\n",
        "\n",
        "(Embedding: Mapping from discrete objects like words to real number vectors)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWDTi282UdUw"
      },
      "source": [
        "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size) :\n",
        "  attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "  attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "  # Training for dynamic rnn decoder\n",
        "  training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
        "                                                                            attention_keys,\n",
        "                                                                            attention_values,\n",
        "                                                                            attention_score_function,\n",
        "                                                                            attention_construct_function,\n",
        "                                                                            name = \"attn_dec_train\"\n",
        "                                                                            ) \n",
        "  decoder_output, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                training_decoder_function,\n",
        "                                                                decoder_embedded_input,\n",
        "                                                                sequence_length,\n",
        "                                                                scope = decoding_scope\n",
        "                                                                )\n",
        "  decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
        "  return output_function(decoder_output_dropout)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yiw6nVVWdjow"
      },
      "source": [
        "### **DECODING THE OBSERVATIONS IN THE TEST/VALIDATION STATE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc7-5Cw3dp2H"
      },
      "source": [
        "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, sequence_length, decoding_scope, output_function, keep_prob, batch_size) :\n",
        "  attention_states = tf.zeroes([batch_size, 1, decoder_cell.output_size])\n",
        "  attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "  # Testing for dynamic rnn decoder\n",
        "  test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
        "                                                                            encoder_state[0],                                                                         encoder_state[0],\n",
        "                                                                            attention_keys,\n",
        "                                                                            attention_values,\n",
        "                                                                            attention_score_function,\n",
        "                                                                            attention_construct_function,\n",
        "                                                                            decoder_embeddings_matrix,\n",
        "                                                                            sos_id,\n",
        "                                                                            eos_id,\n",
        "                                                                            maximum_length,\n",
        "                                                                            num_words,\n",
        "                                                                            name = \"attn_dec_inf\"\n",
        "                                                                            ) \n",
        "  test_predictions, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                  test_decoder_function,\n",
        "                                                                  scope = decoding_scope\n",
        "                                                                  )\n",
        "  return test_predictions"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Utu600hX35"
      },
      "source": [
        "### **CREATING THE DECODING RNN LAYER**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h101mfeMhmhv"
      },
      "source": [
        "def decoder_rnn(decoder_embedded_input, decoder_embedded_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size) :\n",
        "  with tf.variable_scope(\"decoding\") as decoding_scope : \n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "    decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "    # Initialize Weights\n",
        "    weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
        "    # Get the biases\n",
        "    biases = tf.zeros_initializer()\n",
        "    output_function = lambda x : tf.contrib.layers.fully_connected(x,\n",
        "                                                                   num_words,\n",
        "                                                                   None, \n",
        "                                                                   scope = decoding_scope,\n",
        "                                                                   weights_initializer = weights,\n",
        "                                                                   biases_initializer = biases\n",
        "                                                                   )\n",
        "    training_predictions = decode_training_set(encoder_state,\n",
        "                                               decoder_cell,\n",
        "                                               decoder_embedded_input,\n",
        "                                               sequence_length,\n",
        "                                               decoding_scope,\n",
        "                                               output_function,\n",
        "                                               keep_prob,\n",
        "                                               batch_size                                          \n",
        "                                               )\n",
        "    decoding_scope.reuse_variables()\n",
        "    test_predictions = decode_test_set(encoder_state,\n",
        "                                       decoder_cell,\n",
        "                                       decoder_embedded_matrix,\n",
        "                                       word2int['<SOS>'],\n",
        "                                       word2int['<EOS>'],\n",
        "                                       sequence_length-1,\n",
        "                                       num_words,\n",
        "                                       decoding_scope,\n",
        "                                       output_function,\n",
        "                                       keep_prob,\n",
        "                                       batch_size\n",
        "                                       )\n",
        "  return training_predictions, test_predictions"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMBXKmF1lVJO"
      },
      "source": [
        "### **BUILDING THE SEQUENCE TO SEQUENCE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNNZnwYplUtz"
      },
      "source": [
        "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionsWords2int) : \n",
        "  # Putting together the encoder and the decoder\n",
        "  encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs, \n",
        "                                                            answers_num_words+1,\n",
        "                                                            encoder_embedding_size,\n",
        "                                                            initializer = tf.random_uniform_initializer(0,1)\n",
        "                                                            )\n",
        "  encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
        "  preprocessed_targets = preprocess_targets(targets, questionsWords2int, batch_size)\n",
        "  decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words+1, decoder_embedding_size], 0, 1))\n",
        "  decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
        "  training_predictions, test_predictions = decoder_rnn(decoder_embedded_input, \n",
        "                                                       decoder_embedded_matrix, \n",
        "                                                       encoder_state, \n",
        "                                                       num_words, \n",
        "                                                       sequence_length, \n",
        "                                                       rnn_size, \n",
        "                                                       num_layers, \n",
        "                                                       questionsWords2int,\n",
        "                                                       keep_prob, \n",
        "                                                       batch_size\n",
        "                                                       )\n",
        "  return training_predictions, test_predictions"
      ],
      "execution_count": 35,
      "outputs": []
    }
  ]
}